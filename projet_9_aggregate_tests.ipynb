{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import datetime as dt\n",
    "import calendar\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "import plotly.io as pio\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import scipy.stats as scs\n",
    "\n",
    "from sklearn.metrics import r2_score, median_absolute_error,mean_squared_error, mean_squared_log_error, mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('graphiques'):\n",
    "    os.mkdir('graphiques')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eCO2mix_RTE_Annuel-Definitif_2012.csv  eCO2mix_RTE_Annuel-Definitif_2015.csv\r\n",
      "eCO2mix_RTE_Annuel-Definitif_2013.csv  eCO2mix_RTE_Annuel-Definitif_2016.csv\r\n",
      "eCO2mix_RTE_Annuel-Definitif_2014.csv  eCO2mix_RTE_Annuel-Definitif_2017.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls './data/eCO2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Merging dataframe from files<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35136\n",
      "35040\n",
      "35136\n",
      "35040\n",
      "35040\n",
      "35040\n",
      "210432\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Perimetre</th>\n",
       "      <th>Nature</th>\n",
       "      <th>Date</th>\n",
       "      <th>Heures</th>\n",
       "      <th>Consommation</th>\n",
       "      <th>Prevision J-1</th>\n",
       "      <th>Prevision J</th>\n",
       "      <th>Fioul</th>\n",
       "      <th>Charbon</th>\n",
       "      <th>Gaz</th>\n",
       "      <th>Nucleaire</th>\n",
       "      <th>Eolien</th>\n",
       "      <th>Solaire</th>\n",
       "      <th>Hydraulique</th>\n",
       "      <th>Pompage</th>\n",
       "      <th>Bioenergies</th>\n",
       "      <th>Ech. physiques</th>\n",
       "      <th>Taux de Co2</th>\n",
       "      <th>Ech. comm. Angleterre</th>\n",
       "      <th>Ech. comm. Espagne</th>\n",
       "      <th>Ech. comm. Italie</th>\n",
       "      <th>Ech. comm. Suisse</th>\n",
       "      <th>Ech. comm. Allemagne-Belgique</th>\n",
       "      <th>Fioul - TAC</th>\n",
       "      <th>Fioul - Cogen.</th>\n",
       "      <th>Fioul - Autres</th>\n",
       "      <th>Gaz - TAC</th>\n",
       "      <th>Gaz - Cogen.</th>\n",
       "      <th>Gaz - CCG</th>\n",
       "      <th>Gaz - Autres</th>\n",
       "      <th>Hydraulique - Fil de l?eau + eclusee</th>\n",
       "      <th>Hydraulique - Lacs</th>\n",
       "      <th>Hydraulique - STEP turbinage</th>\n",
       "      <th>Bioenergies - Dechets</th>\n",
       "      <th>Bioenergies - Biomasse</th>\n",
       "      <th>Bioenergies - Biogaz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>Donnees definitives</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>00:00</td>\n",
       "      <td>58315.0</td>\n",
       "      <td>58200</td>\n",
       "      <td>58200</td>\n",
       "      <td>492.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3816.0</td>\n",
       "      <td>52697.0</td>\n",
       "      <td>3588.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7922.0</td>\n",
       "      <td>-1139.0</td>\n",
       "      <td>719.0</td>\n",
       "      <td>-9806.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-1750</td>\n",
       "      <td>-1200</td>\n",
       "      <td>-862</td>\n",
       "      <td>-2625</td>\n",
       "      <td>-2940</td>\n",
       "      <td>ND</td>\n",
       "      <td>ND</td>\n",
       "      <td>ND</td>\n",
       "      <td>ND</td>\n",
       "      <td>ND</td>\n",
       "      <td>ND</td>\n",
       "      <td>ND</td>\n",
       "      <td>ND</td>\n",
       "      <td>ND</td>\n",
       "      <td>ND</td>\n",
       "      <td>ND</td>\n",
       "      <td>ND</td>\n",
       "      <td>ND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td>Donnees definitives</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>00:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57700</td>\n",
       "      <td>57550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Perimetre               Nature        Date Heures  Consommation  \\\n",
       "0    France  Donnees definitives  2012-01-01  00:00       58315.0   \n",
       "1    France  Donnees definitives  2012-01-01  00:15           NaN   \n",
       "\n",
       "   Prevision J-1  Prevision J  Fioul  Charbon     Gaz  Nucleaire  Eolien  \\\n",
       "0          58200        58200  492.0     25.0  3816.0    52697.0  3588.0   \n",
       "1          57700        57550    NaN      NaN     NaN        NaN     NaN   \n",
       "\n",
       "   Solaire  Hydraulique  Pompage  Bioenergies  Ech. physiques  Taux de Co2  \\\n",
       "0      0.0       7922.0  -1139.0        719.0         -9806.0         33.0   \n",
       "1      NaN          NaN      NaN          NaN             NaN          NaN   \n",
       "\n",
       "  Ech. comm. Angleterre Ech. comm. Espagne Ech. comm. Italie  \\\n",
       "0                 -1750              -1200              -862   \n",
       "1                   NaN                NaN               NaN   \n",
       "\n",
       "  Ech. comm. Suisse Ech. comm. Allemagne-Belgique Fioul - TAC Fioul - Cogen.  \\\n",
       "0             -2625                         -2940          ND             ND   \n",
       "1               NaN                           NaN         NaN            NaN   \n",
       "\n",
       "  Fioul - Autres Gaz - TAC Gaz - Cogen. Gaz - CCG Gaz - Autres  \\\n",
       "0             ND        ND           ND        ND           ND   \n",
       "1            NaN       NaN          NaN       NaN          NaN   \n",
       "\n",
       "  Hydraulique - Fil de l?eau + eclusee Hydraulique - Lacs  \\\n",
       "0                                   ND                 ND   \n",
       "1                                  NaN                NaN   \n",
       "\n",
       "  Hydraulique - STEP turbinage Bioenergies - Dechets Bioenergies - Biomasse  \\\n",
       "0                           ND                    ND                     ND   \n",
       "1                          NaN                   NaN                    NaN   \n",
       "\n",
       "  Bioenergies - Biogaz  \n",
       "0                   ND  \n",
       "1                  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_raw = pd.DataFrame()\n",
    "tot_length=0\n",
    "for file in os.listdir('data/eCO2'):\n",
    "    df_to_merge=pd.read_csv('./data/eCO2/{}'.format(file), low_memory=False)#set low_memory=False to ignore mixed type in columns (handled later)\n",
    "    length = len(df_to_merge)\n",
    "    print(length)\n",
    "    tot_length +=length\n",
    "    df_raw = df_raw.append(df_to_merge)\n",
    "print(tot_length)\n",
    "df_raw=df_raw.reset_index(drop=True)\n",
    "display(df_raw.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Files (each year) don't have the same number of record of electricity consumption, and some NaN data appear<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Perimetre</th>\n",
       "      <th>Nature</th>\n",
       "      <th>Date</th>\n",
       "      <th>Heures</th>\n",
       "      <th>Consommation</th>\n",
       "      <th>Prevision J-1</th>\n",
       "      <th>Prevision J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>Donnees definitives</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>00:00</td>\n",
       "      <td>58315.0</td>\n",
       "      <td>58200</td>\n",
       "      <td>58200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td>Donnees definitives</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>00:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57700</td>\n",
       "      <td>57550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Perimetre               Nature        Date Heures  Consommation  \\\n",
       "0    France  Donnees definitives  2012-01-01  00:00       58315.0   \n",
       "1    France  Donnees definitives  2012-01-01  00:15           NaN   \n",
       "\n",
       "   Prevision J-1  Prevision J  \n",
       "0          58200        58200  \n",
       "1          57700        57550  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_eCO2=df_raw.iloc[:,0:7].copy()\n",
    "display(df_eCO2.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eCO2['Date']=df_eCO2['Date'].astype('datetime64[ns]')\n",
    "df_eCO2=df_eCO2.sort_values(by=['Date','Heures'],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column full datetime (sec)\n",
    "df_eCO2['Datetime'] = pd.to_datetime(df_eCO2['Date'].apply(str)+' '+df_eCO2['Heures'])\n",
    "df_eCO2['Datetime']=df_eCO2['Datetime'].astype('datetime64[ns]')\n",
    "df_eCO2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_eCO2.dropna().copy()\n",
    "print(len(data))\n",
    "\n",
    "x=data['Datetime']\n",
    "y1=data['Consommation']\n",
    "\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y1,\n",
    "    name = \"Consommation\",\n",
    "    line = dict(color = '#17BECF'),\n",
    "    opacity = 0.8)\n",
    "\n",
    "layout = dict(\n",
    "    title=dict(text=\"<b>Consommation électrique<b>\",\n",
    "                   font=dict(family='Linux Biolinum O',size=18,color='black')),\n",
    "    xaxis=dict(title='Datum',\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=7,\n",
    "                     label='7d',\n",
    "                     step='day',\n",
    "                     stepmode='backward'),\n",
    "                dict(count=3,\n",
    "                     label='3m',\n",
    "                     step='month',\n",
    "                     stepmode='backward'),\n",
    "                dict(step='all')\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(\n",
    "            visible = True\n",
    "        ),\n",
    "        type='date'),\n",
    "    yaxis=dict(title='KW/H')\n",
    ")\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La  variance  ne  semble  pas  toujours  constante autour  de  la  moyenne  saisonnière à  court  terme  mais  elle reste  relativement  stable  sur  des  périodes  plus  longues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comsumption have a year, week and a daily cycle. Called seasonalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also, summer afternoon and winter afternoon are very differents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datepart(df, fldname, drop=False, time=False):\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    \"Add columns relevant to a date in the column `fldname` of `df`.\"\n",
    "    \n",
    "    fld = df[fldname]\n",
    "    fld_dtype = fld.dtype\n",
    "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        fld_dtype = np.datetime64\n",
    "\n",
    "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]atetime$', '', fldname)\n",
    "    #attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            #'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    attr=['Year', 'Month', 'Week', 'Day']\n",
    "    if time: attr = attr + ['Hour', 'Minute', 'Second', 'Dayofweek']\n",
    "    \n",
    "    '''à tester:\n",
    "    -sans la ligne suivante pour enlever cette colonne,\n",
    "    -et de façon isoler pour la corriger'''\n",
    "    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)\n",
    "\n",
    "add_datepart(df_eCO2,'Datetime',time=True)\n",
    "df_eCO2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>NaN Data?<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_eCO2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df_eCO2.isna().sum()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NaN time minutes localisation\n",
    "for quarter in df_eCO2['Minute'].unique():\n",
    "    df=df_eCO2[df_eCO2['Minute']==quarter]\n",
    "    df=pd.DataFrame(df.isna().sum()).T\n",
    "    count=int(df['Consommation'][0])\n",
    "    print(quarter,':',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eCO2.loc[df_eCO2['Consommation'].isnull()].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Every xx:15 and xx:45 records in 'Consomation' are NaN<font>  \n",
    "### <font color='green'>For now, i consider this data are not necessary but to be removed to ligth data. We could eventually estimate them by refering to previsions and consommation.<font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eCO2=df_eCO2.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>In cell n°5 (merging), noted that 2 Dataframes are longer than other. More preciasily:<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#localisation\n",
    "for year in df_eCO2['Year'].unique():\n",
    "    df=df_eCO2[df_eCO2['Year']==year]\n",
    "    l=len(df)\n",
    "    print(year,':',l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#days 17520 \n",
    "(17568-17520)/2/24#because data have 30 mins frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <font color='green'>one day is recorded in 2012 and 2016, but not in 2013,2015,2016,2017\n",
    "#### <font color='red'>Wich days are missing?\n",
    "##### <font color='green'>Pbly in february<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#localisation\n",
    "for year in df_eCO2['Year'].unique():\n",
    "    df=df_eCO2[(df_eCO2['Year']==year)&(df_eCO2['Month']==2.0)]\n",
    "    l=int(len(df))/48\n",
    "    print(year,':',l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in df_eCO2['Year'].unique():\n",
    "    df = df_eCO2[(df_eCO2['Year']==year)&(df_eCO2['Month']==2.0)&\n",
    "                (df_eCO2['Day']==29.0)]\n",
    "    display(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>That's it: years bisextile and not<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>The fact that  2012 and 2016 has one day more can be a source of problems. A specialy for prediction and machine learning<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_eCO2.copy()\n",
    "N = df_eCO2['Dayofweek'].unique()     # boxes (dayofweek)\n",
    "c = ['hsl('+str(h)+',50%'+',50%)' for h in np.linspace(0, 360, len(N))]#color generator\n",
    "# Each box is represented by a dict that contains the data, the type, and the colour. \n",
    "data = [{\n",
    "    'y': data['Consommation'][data['Dayofweek']==m], \n",
    "    'name':str(calendar.day_name[m]),\n",
    "    'type':'box',\n",
    "    'marker':{'color': c[m-1]}\n",
    "    } for m in np.arange(0,len(N),1)]\n",
    "\n",
    "layout = go.Layout(title=dict(text=\"<b>Consommation hebdomadaire<b>\",\n",
    "                   font=dict(family='Linux Biolinum O',size=18,color='black')),\n",
    "                   xaxis=dict(title='Dayofweek',showgrid=False,zeroline=False, tickangle=-45,showticklabels=True),\n",
    "                   yaxis=dict(title='Consommation',gridcolor='white', zeroline=False),\n",
    "                   paper_bgcolor='rgb(233,233,233)',\n",
    "                   plot_bgcolor='rgb(233,233,233)',\n",
    "                  )\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "pyo.iplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_eCO2.copy()\n",
    "N = df_eCO2['Month'].unique()     # boxes (months)\n",
    "c = ['hsl('+str(h)+',50%'+',50%)' for h in np.linspace(0, 360, len(N))]#color generator\n",
    "# Each box is represented by a dict that contains the data, the type, and the colour. \n",
    "data = [{\n",
    "    'y': data['Consommation'][data['Month']==m], \n",
    "    'name':str(calendar.month_name[m]),\n",
    "    'type':'box',\n",
    "    'marker':{'color': c[m-1]}\n",
    "    } for m in np.arange(0,len(N)+1,1)]\n",
    "\n",
    "layout = go.Layout(title=dict(text=\"<b>Consommation mensuelle<b>\",\n",
    "                   font=dict(family='Linux Biolinum O',size=18,color='black')),\n",
    "                   xaxis=dict(title='Months',showgrid=False,zeroline=False, tickangle=-45,showticklabels=True),\n",
    "                   yaxis=dict(title='Consommation',gridcolor='white', zeroline=False),\n",
    "                   paper_bgcolor='rgb(233,233,233)',\n",
    "                   plot_bgcolor='rgb(233,233,233)',\n",
    "                  )\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "pyo.iplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_eCO2.copy()\n",
    "N = df_eCO2['Year'].unique()     # boxes (years)\n",
    "c = ['hsl('+str(h)+',50%'+',50%)' for h in np.linspace(0, 360, len(N))]#color generator\n",
    "# Each box is represented by a dict that contains the data, the type, and the colour. \n",
    "data = [{\n",
    "    'y': data['Consommation'][data['Year']==m], \n",
    "    'name':str(m),\n",
    "    'type':'box',\n",
    "    'marker':{'color': c[m-2012-1]}\n",
    "    } for m in list(N)]\n",
    "\n",
    "layout = go.Layout(title=dict(text=\"<b>Consommation annuelle<b>\",\n",
    "                   font=dict(family='Linux Biolinum O',size=18,color='black')),\n",
    "                   xaxis=dict(title='Year',showgrid=False,zeroline=False, tickangle=-45,showticklabels=True),\n",
    "                   yaxis=dict(title='Consommation',gridcolor='white', zeroline=False),\n",
    "                   paper_bgcolor='rgb(233,233,233)',\n",
    "                   plot_bgcolor='rgb(233,233,233)',\n",
    "                  )\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "pyo.iplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_eCO2.copy()\n",
    "years = list(data['Year'].unique())\n",
    "\n",
    "x=data['Datetime']\n",
    "y1=data['Consommation']\n",
    "y2=data['Prevision J-1']\n",
    "y3=data['Prevision J']\n",
    "\n",
    "\n",
    "trace1 = go.Line(x=x,\n",
    "                  y=y1,\n",
    "                  opacity = 1,\n",
    "                  name='Consommation',\n",
    "                  marker=dict(color='green'))\n",
    "\n",
    "trace2 = go.Line(x=x,\n",
    "                  y=y2,\n",
    "                  opacity = 0.6,\n",
    "                  name='Prevision J-1',\n",
    "                  marker=dict(color='blue'))\n",
    "\n",
    "trace3 = go.Line(x=x,\n",
    "                  y=y3,\n",
    "                  opacity = 0.6,\n",
    "                  name='Prevision J',\n",
    "                  marker=dict(color='red'))\n",
    "\n",
    "data = [trace1,trace2,trace3]\n",
    "\n",
    "layout = dict(\n",
    "    title=dict(text=\"<b>Consommation et prédictions<b>\",\n",
    "               font=dict(family='Linux Biolinum O',size=18,color='black')),\n",
    "    xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1,\n",
    "                     label='1m',\n",
    "                     step='month',\n",
    "                     stepmode='backward'),\n",
    "                dict(count=6,\n",
    "                     label='6m',\n",
    "                     step='month',\n",
    "                     stepmode='backward'),\n",
    "                dict(count=1,\n",
    "                    label='1y',\n",
    "                    step='year',\n",
    "                    stepmode='backward'),\n",
    "                dict(step='all')\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(\n",
    "            visible = True\n",
    "        ),\n",
    "        type='date'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=df_eCO2.copy()\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "x=data['Datetime']\n",
    "y1=data['Consommation']-data['Prevision J-1']\n",
    "y2=data['Consommation']-data['Prevision J']\n",
    "\n",
    "\n",
    "trace1 = go.Line(x=x,\n",
    "                  y=y1,\n",
    "                  name='Consommation-prévision J-1',\n",
    "                  marker=dict(color='red'))\n",
    "\n",
    "trace2 = go.Line(x=x,\n",
    "                  y=y2,\n",
    "                  name='Consommation-prévision J',\n",
    "                  marker=dict(color='blue'))\n",
    "\n",
    "data = [trace1,trace2]\n",
    "\n",
    "layout = go.Layout(title=dict(text=\"<b><b>\",\n",
    "                              font=dict(family='Linux Biolinum O',size=18,color='black')),\n",
    "                   xaxis=dict(title='Datum'),\n",
    "                   yaxis=dict(title=''))\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "pyo.iplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prevision J',df_eCO2['Month'][df_eCO2['Prevision J']==0].unique())\n",
    "print('Prevision J',df_eCO2['Year'][df_eCO2['Prevision J']==0].unique())\n",
    "print('Prevision J-1',df_eCO2['Month'][df_eCO2['Prevision J-1']==0].unique())\n",
    "print('Prevision J-1',df_eCO2['Year'][df_eCO2['Prevision J-1']==0].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eCO2['Prevision J'][df_eCO2['Prevision J']==0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>94 forecasts ('Prevision J') for the month of March 2015 are inconsistent. It is opportune, considering the prediction purpose, to correct this by referring to the predictions of March of other years. <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eCO2.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate datetime if predicition is <0 then list day and month\n",
    "#and return mean prediction from other years, at the same time\n",
    "list_mean=[]\n",
    "list_datetime_no_prediction=list(df_eCO2['Datetime'][df_eCO2['Prevision J']==0])\n",
    "#extract day month and time\n",
    "for dt in list_datetime_no_prediction:\n",
    "    day=int(df_eCO2.Day[df_eCO2['Datetime']==dt].values)\n",
    "    month=int(df_eCO2.Month[df_eCO2['Datetime']==dt].values)\n",
    "    tim=str(df_eCO2.Heures[df_eCO2['Datetime']==dt].values)\n",
    "    tim=tim[2:7]\n",
    "    i = int(df_eCO2.index[df_eCO2['Datetime']==dt].values)\n",
    "    mean = df_eCO2['Prevision J'][(df_eCO2['Year'].isin(['2012','2013','2014','2016','2017']))&\n",
    "                 (df_eCO2['Day']==day)&\n",
    "                 (df_eCO2['Month']==month)&\n",
    "                 (df_eCO2['Heures']==tim)\n",
    "                  ].mean()\n",
    "    df_eCO2.at[i,'Prevision J']=mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eCO2['Prevision J'][df_eCO2['Prevision J']==0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=df_eCO2.dropna().reset_index(drop=True)\n",
    "rec_day = len(df_eCO2)/len(df_eCO2['Date'].unique())\n",
    "week_i = 7*rec_day#week interval\n",
    "r = randint(a=week_i,b=len(data)-week_i)\n",
    "years = list(data['Year'].unique())\n",
    "data = data[['Date','Datetime','Heures','Consommation','Prevision J-1','Prevision J']].loc[r-week_i/2:r+week_i/2,:]\n",
    "\n",
    "x=data['Datetime']\n",
    "y1=data['Consommation']\n",
    "y2=data['Prevision J-1']\n",
    "y3=data['Prevision J']\n",
    "\n",
    "\n",
    "trace1 = go.Line(x=x,\n",
    "                  y=y1,\n",
    "                  opacity = 1,\n",
    "                  name='Consommation',\n",
    "                  marker=dict(color='green'))\n",
    "\n",
    "trace2 = go.Line(x=x,\n",
    "                  y=y2,\n",
    "                  opacity = 0.6,\n",
    "                  name='Prevision J-1',\n",
    "                  marker=dict(color='blue'))\n",
    "\n",
    "trace3 = go.Line(x=x,\n",
    "                  y=y3,\n",
    "                  opacity = 0.6,\n",
    "                  name='Prevision J',\n",
    "                  marker=dict(color='red'))\n",
    "\n",
    "data = [trace1,trace2,trace3]\n",
    "\n",
    "layout = dict(\n",
    "    title=dict(text=\"<b>Consommation et prédictions, échelles plus fines & période aléatoire<b>\",\n",
    "               font=dict(family='Linux Biolinum O',size=18,color='black')),\n",
    "    xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1,\n",
    "                     label='1d',\n",
    "                     step='day',\n",
    "                     stepmode='backward'),\n",
    "                dict(count=7,\n",
    "                     label='7d',\n",
    "                     step='day',\n",
    "                     stepmode='backward'),\n",
    "                dict(step='all')\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(\n",
    "            visible = True\n",
    "        ),\n",
    "        type='date'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### seasons\n",
    "data=df_eCO2.dropna().reset_index(drop=True)\n",
    "data=data.groupby(by=['Month','Day','Heures'], as_index=False).mean()\n",
    "x=data['Heures'].unique()\n",
    "summer=data['Consommation'][(data['Day']==15)&(data['Month']==7)]\n",
    "spring=data['Consommation'][(data['Day']==15)&(data['Month']==4)]\n",
    "fall=data['Consommation'][(data['Day']==30)&(data['Month']==10)]\n",
    "winter=data['Consommation'][(data['Day']==1)&(data['Month']==1)]\n",
    "\n",
    "trace1 = go.Line(x=x,\n",
    "                  y=summer,\n",
    "                  opacity = 1,\n",
    "                  name='Ete 15-07',\n",
    "                  marker=dict(color='red'))\n",
    "\n",
    "trace2 = go.Line(x=x,\n",
    "                  y=spring,\n",
    "                  opacity = 0.6,\n",
    "                  name='Printemps 15-04',\n",
    "                  marker=dict(color='green'))\n",
    "\n",
    "trace3 = go.Line(x=x,\n",
    "                  y=fall,\n",
    "                  opacity = 0.6,\n",
    "                  name='Automne 30-10',\n",
    "                  marker=dict(color='black'))\n",
    "\n",
    "trace4 = go.Line(x=x,\n",
    "                  y=winter,\n",
    "                  opacity = 0.6,\n",
    "                  name='Hiver 01-01',\n",
    "                  marker=dict(color='blue'))\n",
    "\n",
    "data = [trace1,trace2,trace3,trace4]\n",
    "\n",
    "layout = go.Layout(title=dict(text=\"<b>Consommation sur 24h selon la saison<b>\",\n",
    "                              font=dict(family='Linux Biolinum O',size=18,color='black')),\n",
    "                   xaxis=dict(title='Datum'),\n",
    "                   yaxis=dict(title='KW/H'))\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daylike has some specificities (daymean and evening) regarding to the season but general rythmus is almost the same:  \n",
    "Consommation raises arround 5 am until noon, then drops (more or less) during after-noon, reraise arround 17:30 and appears some specificities during the evening, finally drops until morning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week and season point of view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### week seasons\n",
    "data=df_eCO2.dropna().reset_index(drop=True)\n",
    "data=data.groupby(by=['Month','Dayofweek','Heures'], as_index=False).mean()\n",
    "\n",
    "x=[]\n",
    "hours=df_eCO2['Heures'].unique()\n",
    "daysweeks=[calendar.day_name[m] for m in np.arange(0,7,1)]\n",
    "for d in daysweeks:\n",
    "    for h in hours:\n",
    "        dh=str(d+' '+h)\n",
    "        x.append(dh)\n",
    "        \n",
    "summer=data['Consommation'][data['Month']==7]\n",
    "spring=data['Consommation'][data['Month']==4]\n",
    "fall=data['Consommation'][data['Month']==10]\n",
    "winter=data['Consommation'][data['Month']==1]\n",
    "\n",
    "trace1 = go.Line(x=x,\n",
    "                  y=summer,\n",
    "                  opacity = 1,\n",
    "                  name='July',\n",
    "                  marker=dict(color='red'))\n",
    "\n",
    "trace2 = go.Line(x=x,\n",
    "                  y=spring,\n",
    "                  opacity = 0.6,\n",
    "                  name='April',\n",
    "                  marker=dict(color='green'))\n",
    "\n",
    "trace3 = go.Line(x=x,\n",
    "                  y=fall,\n",
    "                  opacity = 0.8,\n",
    "                  name='October',\n",
    "                  marker=dict(color='black'))\n",
    "\n",
    "trace4 = go.Line(x=x,\n",
    "                  y=winter,\n",
    "                  opacity = 0.6,\n",
    "                  name='January',\n",
    "                  marker=dict(color='blue'))\n",
    "\n",
    "data = [trace1,trace2,trace3,trace4]\n",
    "\n",
    "layout = go.Layout(title=dict(text=\"<b>Consommation sur une semaine selon la saison<b>\",\n",
    "                              font=dict(family='Linux Biolinum O',size=18,color='black')),\n",
    "                   xaxis=dict(title='Datum',\n",
    "                              type='-',\n",
    "                              nticks = 7,\n",
    "                              autorange=True),\n",
    "                   yaxis=dict(title='KW/H'))\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Weekday and Week-end seems different whatever the season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_eCO2=df_eCO2[df_eCO2['Heures']=='14:00']\n",
    "df_eCO2=df_eCO2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eCO2=df_eCO2.drop(columns='Elapsed')\n",
    "df_eCO2.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datetime has been lost in the battel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='orange'>Heat correction of the consumption data<font>\n",
    "#### DJU are localy and monthly recorded. So we'll focus on NICE\n",
    "The heat fixing consists of removing the extr, whatever the harshness of the winter. By reducing the heating consumption to a reference climate, characterized by the DJU, it removes the variations due to the climatic severity. Kind of outlier removal.  \n",
    "formula:  \n",
    "Cconsumption = consumption x (DJU_reference/DJU_day)  \n",
    "    with:  \n",
    "    DJU_reference = 18  \n",
    "          DJU_day = download  \n",
    "          consumption = download\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw=pd.read_csv('./data/dju_nice.csv')\n",
    "display(df_raw.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>Preparing for merging<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_months=list(df_raw.columns.unique()[1:13])\n",
    "l_years = list(df_raw['Annee'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DJU=pd.DataFrame()\n",
    "df_DJU['Year']=np.repeat(l_years,len(l_months))\n",
    "df_DJU['Month_l']=np.tile(l_months,len(l_years))\n",
    "df_DJU['Month']=np.tile(np.arange(1,13,1),len(l_years))\n",
    "df_DJU.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "l_djus=[]\n",
    "for y,m in product(l_years,l_months):\n",
    "    df = df_raw[df_raw['Annee']==y].copy()\n",
    "    dju = float(df[[m]].values)\n",
    "    l_djus.append(dju)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DJU['dju']=l_djus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DJU.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_DJU.copy()\n",
    "data = data.groupby(by=['Year','Month_l'],as_index=False).mean()\n",
    "data = data.sort_values(by=['Year','Month'])\n",
    "\n",
    "liste_m_y=[]\n",
    "for y in l_years:\n",
    "    for m in l_months:\n",
    "        d = str(y) + str(m)\n",
    "        liste_m_y.append(d)\n",
    "\n",
    "x=liste_m_y\n",
    "y1=data['dju']\n",
    "trace1 = go.Line(x=x,\n",
    "                  y=y1,\n",
    "                  name='DJU mensuel',\n",
    "                  marker=dict(color='red'))\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(title=dict(text=\"<b>Nice 2012--2018 (1)<b>\",\n",
    "                              font=dict(family='Linux Biolinum O',size=18,color='black')),\n",
    "                   xaxis=dict(title='Datum'),\n",
    "                   yaxis=dict(title='DJU', ),\n",
    "                   paper_bgcolor='rgb(233,233,233)',\n",
    "                   plot_bgcolor='rgb(233,233,233)',)\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "pyo.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p9 = df_eCO2.merge(df_DJU, on=['Year','Month'])\n",
    "df_p9.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_p9.copy()\n",
    "data = data.groupby(by=['Year','Month'],as_index=False).mean()\n",
    "data = data.sort_values(by=['Year','Month'])\n",
    "data=data.reset_index(drop=True)\n",
    "print(len(data))\n",
    "\n",
    "x=liste_m_y\n",
    "y1=data['Consommation']\n",
    "y2=data['dju']\n",
    "y3=df_p9['Consommation']\n",
    "\n",
    "\n",
    "trace1 = go.Line(x=x,\n",
    "                  y=y1,\n",
    "                  opacity = 0.8,\n",
    "                  name='Consommation',\n",
    "                  marker=dict(color='green'))\n",
    "\n",
    "trace2 = go.Line(x=x,\n",
    "                  y=y2,\n",
    "                  opacity = 0.8,\n",
    "                  name='DJU',\n",
    "                  marker=dict(color='red'),\n",
    "                  yaxis='y2')\n",
    "\n",
    "data = [trace1,trace2]\n",
    "\n",
    "layout = go.Layout(title=dict(text=\"<b>Concordance entre la consommation électrique mensuelle et les DJU<b>\",\n",
    "                              font=dict(family='Linux Biolinum O',size=18,color='black')),\n",
    "                   xaxis=dict(title='Datum'),\n",
    "                   yaxis=dict(title='KW/h'),\n",
    "                   yaxis2=dict(\n",
    "                            title='dju',\n",
    "                            titlefont=dict(\n",
    "                                color='rgb(148, 103, 189)'\n",
    "                            ),\n",
    "                            tickfont=dict(\n",
    "                                color='rgb(148, 103, 189)'\n",
    "                            ),\n",
    "                            overlaying='y',\n",
    "                            side='right'),\n",
    "                   paper_bgcolor='rgb(233,233,233)',\n",
    "                   plot_bgcolor='rgb(233,233,233)',\n",
    "                  )\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "pyo.iplot(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGGREGATION(week) to make data ligther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p9_agg_month=df_p9.copy().groupby(by=['Year','Month'], as_index=False,).mean()\n",
    "df_p9_agg_month=df_p9_agg_month.reset_index(drop=True)\n",
    "\n",
    "df_p9_agg_day=df_p9.copy().groupby(by=['Date'], as_index=False).mean()\n",
    "df_p9_agg_day['Datetime']=df_p9_agg_day['Date'].astype('datetime64[ns]')\n",
    "df_p9_agg_day=df_p9_agg_day.set_index('Datetime',drop=True)\n",
    "\n",
    "df_p9_agg_hour=df_p9[df_p9['Minute']==0].copy()\n",
    "df_p9_agg_hour=df_p9_agg_hour.reset_index(drop=True)\n",
    "df_p9_agg_hour['Datetime'] = pd.to_datetime(df_p9_agg_hour['Date'].apply(str)+' '+df_p9_agg_hour['Heures'])\n",
    "df_p9_agg_hour=df_p9_agg_hour.set_index('Datetime',drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_p9_agg_month),len(df_p9_agg_day),len(df_p9_agg_hour),len(df_p9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='violet'>Heat correction via linear regression<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear correction will pbly year deseasonal the consommation no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p9_agg_month.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p9_agg_day.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p9_agg_hour.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p9_agg_hour['Correction_consommation']=smf.ols(formula='Consommation ~ dju',\n",
    "                                                 data=df_p9_agg_hour).fit().resid\n",
    "\n",
    "df_p9_agg_day['Correction_consommation']=smf.ols(formula='Consommation ~ dju',\n",
    "                                                 data=df_p9_agg_day).fit().resid\n",
    "\n",
    "df_p9_agg_month['Correction_consommation']=smf.ols(formula='Consommation ~ dju',\n",
    "                                                 data=df_p9_agg_month).fit().resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lin = smf.ols(formula='Consommation ~ dju', data=df_p9_agg_hour).fit()\n",
    "reg_lin.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>SARIMA(p,d,q) (P,D,Q,s)<font>  \n",
    "    p:AR, AutoRegressiv term making value depends from p past values  \n",
    "    d:I, Differences  \n",
    "    q:MA  Movingterm Average extracting residuals MA(original data)\n",
    "    P:AR (for seasonal component)  \n",
    "    D:I (for seasonal component)  \n",
    "    Q:MA (for seasonal component)  \n",
    "    s:seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARIMAtest(series_train,series_test,p,d,q,sp,sd,sq,s):\n",
    "    sarima = sm.tsa.statespace.SARIMAX(train,order=(p,d,q),seasonal_order=(sp,sd,sq,s),\n",
    "                                enforce_stationarity=False, enforce_invertibility=False).fit()\n",
    "    pred = sarima.predict(train_end,test_end)[1:]\n",
    "    print('SARIMA model MSE:{}'.format(mean_squared_error(test,pred)))\n",
    "    pd.DataFrame({'test':test,'pred':pred}).plot();\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(df_p9_agg_month.Correction_consommation, lags=25, alpha=.05, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(df_p9_agg_month.Correction_consommation, lags=25, alpha=.05, ax=ax2)\n",
    "print(\"ACF and PACF:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_p9_agg_month.copy()\n",
    "data['Correction_consommation']=data['Correction_consommation']\n",
    "data=data.reset_index(drop=True)\n",
    "train_start,train_end = 0,60\n",
    "test_start,test_end = 60,72\n",
    "train = data['Correction_consommation'][train_start:train_end].dropna() \n",
    "test = data['Correction_consommation'][test_start:test_end].dropna()\n",
    "SARIMAtest(train,test,1,0,1,1,1,1,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resDiff = sm.tsa.arma_order_select_ic(train,max_ar=2, max_ma=2, ic='aic', trend='nc')\n",
    "print('ARMA(p,q) =',resDiff['aic_min_order'],'is the best.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_p9_agg_day.copy()\n",
    "data['Correction_consommation']\n",
    "train_start,train_end = '2012-01-01','2016-12-31'\n",
    "test_start,test_end = '2017-01-01','2017-12-31'\n",
    "train = data['Correction_consommation'][train_start:train_end].dropna()\n",
    "test = data['Correction_consommation'][test_start:test_end].dropna()\n",
    "SARIMAtest(train,test,1,0,1,1,1,1,365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resDiff = sm.tsa.arma_order_select_ic(train, max_ar=2, max_ma=2, ic='aic', trend='nc')\n",
    "print('ARMA(p,q) =',resDiff['aic_min_order'],'is the best.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_p9_agg_hour.copy()\n",
    "data['Correction_consommation']\n",
    "train_start,train_end = '2012-01-01 00:00:00','2016-12-31 23:00:00'\n",
    "test_start,test_end = '2017-01-01 00:00:00','2017-12-31 23:00:00'\n",
    "train = data['Correction_consommation'][train_start:train_end].dropna()\n",
    "test = data['Correction_consommation'][test_start:test_end].dropna()\n",
    "SARIMAtest(train,test,1,0,1,0,0,0,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = arima.resid\n",
    "fig,ax = plt.subplots(2,1,figsize=(15,8))\n",
    "fig = sm.graphics.tsa.plot_acf(res, lags=25, ax=ax[0])\n",
    "fig = sm.graphics.tsa.plot_pacf(res, lags=25, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='Orange'>Exponential smoothing<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " instead of only weighting the time series' last k values, we would weight all available observations while exponentially decreasing the weights as we move further back in time.  \n",
    " We can think of α as the smoothing factor or memory decay rate, it defines how quickly we will \"forget\" the last available true observation. The smaller α is, the more influence the previous observations have and the smoother the series is. In other words, the higher the α\n",
    "\n",
    ", the faster the method \"forgets\" about the past.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y^x=α⋅yx+(1−α)⋅y^x−1, we can see that in order to make the prediction for y^x we also need to have the observed value yx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='violet'>Simple exponential smoothing<font>  AR(p)I(d)MA(q)    (seasonality, trend, and noise)\n",
    "    (to weight observations by their age)  \n",
    "Prediction = Prédiction faite à l’instant précédent corrigée par un terme proportionnel à l’erreur de prévision correspondante.  \n",
    "#### Effective for stationary data points: Irregular data, No seasonality or trend. The initial value to start the smooth have to be determined (the first of the serie, a mean,...).  \n",
    "If alpha = 1: naive methode: next = previous  \n",
    "If alpha = 0: mean methode: next = mean(previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smoothingzoom(series, alpha, zoom):\n",
    "    \"\"\"\n",
    "        next value is predicted by the previous one, \"fixed\" by alpha as : x(p) = x(p-1) + alpha*(ap(-1))\n",
    "        series - dataset with timestamps\n",
    "        alpha - float [0.0, 1.0], smoothing parameter proportional with the error made at previous prediction\n",
    "    \"\"\"\n",
    "    end=zoom+1\n",
    "    # first value is same as series\n",
    "    result = [series[0]]\n",
    "    end=zoom+1\n",
    "    for n in range(1, end):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def plotExponentialSmoothingzoom(series, alphas, zooms):\n",
    "    \"\"\"\n",
    "        Plots exponential smoothing with different alphas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters\n",
    "        \n",
    "    \"\"\"\n",
    "    with plt.style.context('seaborn-white'):\n",
    "        for zoom in zooms:\n",
    "            end=zoom+1\n",
    "\n",
    "            plt.figure(figsize=(15, 7))\n",
    "            for alpha in alphas:\n",
    "                plt.plot(exponential_smoothingzoom(series, alpha,zoom), label=\"Alpha {}\".format(alpha))\n",
    "            plt.plot(series[:end].values, 'green', alpha=0.4,label = \"Actual\")\n",
    "            plt.legend(loc=\"best\")\n",
    "            plt.axis('tight')\n",
    "            plt.title(\"Exponential Smoothing\\nfzoom = {} day(s)\".format(zoom))\n",
    "            plt.grid(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_p9_agg_hour.copy()\n",
    "plotExponentialSmoothingzoom(data.Correction_consommation, [0.2, 0.9],zooms=freqcy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='green'>Of course, alpha=0.9 shapes pretty cloth to actual:  <font>\n",
    "    -high alpha gives more importance to recent data.\n",
    "    -series has no trend so SES is efficient.\n",
    "#### the daily seasonality seems caugth by the SMA(0.9): with a step 1, record(x) ~= record(x+1), with hour frequency. In fact, the daily seasonality is probably not caugth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Double exponential smoothing<font>  \n",
    "### More efficient for data having trend(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if our time series has a trend, we can incorporate that information to do better than just estimating the current level and using that to forecast the future observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### zoom\n",
    "def double_exponential_smoothingzoom(series, alpha, beta, zoom):\n",
    "    \"\"\"\n",
    "        series - dataset with timeseries\n",
    "        alpha - float [0.0, 1.0], smoothing parameter for level\n",
    "        beta - float [0.0, 1.0], smoothing parameter for trend\n",
    "    \"\"\"\n",
    "        \n",
    "    end=zoom+1\n",
    "    # first value is same as series\n",
    "    result = [series[0]]\n",
    "    end=zoom+1\n",
    "    for n in range(1, end):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= end: # forecasting\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n",
    "        trend = beta*(level-last_level) + (1-beta)*trend\n",
    "        result.append(level+trend)\n",
    "    return result\n",
    "\n",
    "def plotDoubleExponentialSmoothingzoom(series, alphas, betas, zooms):\n",
    "    \"\"\"\n",
    "        Plots double exponential smoothing with different alphas and betas\n",
    "        \n",
    "        series - dataset with timestamps\n",
    "        alphas - list of floats, smoothing parameters for level\n",
    "        betas - list of floats, smoothing parameters for trend\n",
    "    \"\"\"\n",
    "    with plt.style.context('seaborn-white'):\n",
    "        for zoom in zooms:\n",
    "            end=zoom+1\n",
    "\n",
    "            plt.figure(figsize=(20, 8))\n",
    "            for alpha in alphas:\n",
    "                for beta in betas:\n",
    "                    plt.plot(double_exponential_smoothingzoom(series, alpha, beta, zoom), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "            plt.plot(series[:end].values, label = \"Actual\")\n",
    "            plt.legend(loc=\"best\")\n",
    "            plt.axis('tight')\n",
    "            plt.title(\"Double Exponential Smoothing\\nzoom = {} day(s)\".format(zoom))\n",
    "            plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=df_p9_first_correction.reset_index(drop=True)\n",
    "plotDoubleExponentialSmoothingzoom(data.Correction_consommation,alphas=[0.9, 0.02], betas=[0.9, 0.02],zooms=[7,30,365])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this method can now predictive future values, if we stare closer at the forecast formula y^x+1=ℓx+kTx, we can see that once the trend (T) is estimated to be positive, all future predictions can only go up from the last value in the time series. On the other hand, if the trend (T) is estimated to be negative, all future predictions can only go down. This property makes this method unsuitable for predicting very far out into the future as well. With that in mind, let's now turn towards triple exponential smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>2 factors exponantial smoothing is interessing. Aspecialy with:  <font>\n",
    "#### <font color='blue'>-high alpha and high beta (blue)<font>  \n",
    "#### <font color='orange'>-high alpha and low beta (orange)<font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='violet'>Tuning double exponential smoothing<font>\n",
    "#### Alpha and Beta need to be tuned.  \n",
    "#### *Alpha smooths the serie around the trend,  \n",
    "#### *Beta smooth the trend itself.  \n",
    "      \n",
    "#### The higher they are, the more weight the most recent observations will have,     \n",
    "#### and the less smoothed the serie extracted from the model will be.  \n",
    "  \n",
    "#### This tune will be done automaticaly (by refering to an accuracy rating (loss function) as MSE, SSE, etc...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Triple exponential smoothing a.k.a. Holt-Winters<font>  \n",
    "    Main idear: add seasonal component to our model.  \n",
    "#### Suitable for univariate time series with trend and/or seasonal components. \n",
    "#### We saw earlier that series as many seasonabilities.  \n",
    "    Then the seasonal component of the 3rd point into the season would be exponentially smoothed with the 3rd point of last season, 3rd point two seasons ago, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://annals-csis.org/proceedings/2012/pliks/118.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoltWinters:\n",
    "    \n",
    "    \"\"\"\n",
    "    Holt-Winters model with the anomalies detection using Brutlag method\n",
    "    \n",
    "    # series - initial time series\n",
    "    # slen - length of a season -- seasonality\n",
    "    # alpha, beta, gamma - Holt-Winters model coefficients\n",
    "    # n_preds - predictions horizon -- refering to the time series periodicity\n",
    "    # scaling_factor - sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n",
    "        self.series = series\n",
    "        self.slen = slen#season length\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.n_preds = n_preds\n",
    "        self.scaling_factor = scaling_factor\n",
    "        \n",
    "        \n",
    "    def initial_trend(self):\n",
    "        \"\"\"Common method to initiate the trend: the trend averages across seasons\"\"\"\n",
    "        sum = 0.0\n",
    "        for i in range(self.slen):\n",
    "            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n",
    "        return sum / self.slen  \n",
    "    \n",
    "    def initial_seasonal_components(self):\n",
    "        seasonals = {}\n",
    "        season_averages = []\n",
    "        n_seasons = int(len(self.series)/self.slen)\n",
    "        # let's calculate season averages\n",
    "        for j in range(n_seasons):\n",
    "            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n",
    "        # let's calculate initial values\n",
    "        for i in range(self.slen):\n",
    "            sum_of_vals_over_avg = 0.0\n",
    "            for j in range(n_seasons):\n",
    "                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n",
    "            seasonals[i] = sum_of_vals_over_avg/n_seasons\n",
    "        return seasonals   \n",
    "\n",
    "          \n",
    "    def triple_exponential_smoothing(self):\n",
    "        self.result = []#ante prediction values smoothed and predictions\n",
    "        self.Smooth = []#all values smoothed\n",
    "        self.Season = []\n",
    "        self.Trend = []\n",
    "        self.PredictedDeviation = []\n",
    "        self.UpperBond = []\n",
    "        self.LowerBond = []\n",
    "        \n",
    "        seasonals = self.initial_seasonal_components()\n",
    "        \n",
    "        for i in range(len(self.series)+self.n_preds):\n",
    "            if i == 0: # components initialization: it can't be predict and first observation of time series is used \n",
    "                smooth = self.series[0]\n",
    "                trend = self.initial_trend()\n",
    "                self.result.append(self.series[0])\n",
    "                self.Smooth.append(smooth)\n",
    "                self.Trend.append(trend)\n",
    "                self.Season.append(seasonals[i%self.slen])\n",
    "                \n",
    "                self.PredictedDeviation.append(0)\n",
    "                \n",
    "                self.UpperBond.append(self.result[0] + \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "                \n",
    "                self.LowerBond.append(self.result[0] - \n",
    "                                      self.scaling_factor * \n",
    "                                      self.PredictedDeviation[0])\n",
    "                continue\n",
    "                \n",
    "            if i >= len(self.series): # predicting session begins\n",
    "                m = i - len(self.series) + 1\n",
    "                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n",
    "                \n",
    "                # when predicting we increase uncertainty on each step\n",
    "                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n",
    "                \n",
    "            else:#calculated deviation for each value in the time serie beween first value of the serie and prediction start\n",
    "                val = self.series[i]\n",
    "                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n",
    "                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n",
    "                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n",
    "                self.result.append(smooth+trend+seasonals[i%self.slen])\n",
    "                \n",
    "                # Deviation is calculated according to Brutlag algorithm.\n",
    "                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n",
    "                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n",
    "            #confindence boundary          \n",
    "            self.UpperBond.append(self.result[-1] + \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.LowerBond.append(self.result[-1] - \n",
    "                                  self.scaling_factor * \n",
    "                                  self.PredictedDeviation[-1])\n",
    "\n",
    "            self.Smooth.append(smooth)\n",
    "            self.Trend.append(trend)\n",
    "            self.Season.append(seasonals[i%self.slen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='green'>Cross validation to estimate best parameters value<font>\n",
    "### NEED TO TAKE CARE ABOUT TIME LOGICAL so previous questions are:  \n",
    "    Length of the duration model will begin to train on?  \n",
    "    Length of prediction step?  \n",
    "####    Note that next fold of the cross validation will train on data first train + prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''TimeSeriesSplit is very interessing for time series:\n",
    "training fold1 has x values,\n",
    "trainging fold2 has fold1 values + next x values\n",
    ".\n",
    ".\n",
    ".\n",
    "testing fold1 has the next x values from traing fold 1\n",
    "testing fold2 has the next x values from traing fold 2\n",
    ".\n",
    ".\n",
    ".\n",
    "HOWEVER: n_folds can be limited'''\n",
    "\n",
    "def timeseriesCVscore(params, series, loss_function=mean_squared_error, slen=365):\n",
    "    \"\"\"\n",
    "        Returns error on CV  \n",
    "        \n",
    "        params - vector of parameters for optimization\n",
    "        series - dataset with timeseries\n",
    "        slen - season length for Holt-Winters model\n",
    "    \"\"\"\n",
    "     # errors array\n",
    "    errors = []\n",
    "    \n",
    "    values = series.values\n",
    "    alpha, beta, gamma = params\n",
    "    \n",
    "    # set the number of folds for cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=2) \n",
    "    \n",
    "    # iterating over folds, train model on each, forecast and calculate error\n",
    "    for train, test in tscv.split(values):\n",
    "\n",
    "        model = HoltWinters(series=values[train], slen=slen, \n",
    "                            alpha=alpha, beta=beta, gamma=gamma, n_preds=len(test))\n",
    "        model.triple_exponential_smoothing()\n",
    "        \n",
    "        predictions = model.result[-len(test):]\n",
    "        actual = values[test]\n",
    "        error = loss_function(predictions, actual)\n",
    "        errors.append(error)\n",
    "        \n",
    "    return np.mean(np.array(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "data=df_p9_first_correction.Correction_consommation.copy()\n",
    "\n",
    "# initializing model parameters alpha, beta and gamma\n",
    "x = [0, 0, 0] \n",
    "\n",
    "# Minimizing the loss function => optimizing algo\n",
    "opt = minimize(timeseriesCVscore, x0=x, \n",
    "               args=(data, mean_squared_error), \n",
    "               method='TNC', bounds = ((0, 1), (0, 1), (0, 1))\n",
    "              )\n",
    "\n",
    "# Take optimal values...\n",
    "alpha_final, beta_final, gamma_final = opt.x\n",
    "print(alpha_final, beta_final, gamma_final)\n",
    "\n",
    "# ...and train the model with them, forecasting for the 30 next freq\n",
    "model = HoltWinters(series=data, slen=365,\n",
    "                    alpha = alpha_final, \n",
    "                    beta = beta_final, \n",
    "                    gamma = gamma_final, \n",
    "                    n_preds = 365, \n",
    "                    scaling_factor = 1)\n",
    "model.triple_exponential_smoothing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHoltWinters(series, slen, plot_intervals=False, plot_anomalies=False):\n",
    "    \"\"\"\n",
    "        series - dataset with timeseries\n",
    "        plot_intervals - show confidence intervals\n",
    "        plot_anomalies - show anomalies \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(model.result, '-',label = \"Model\",linewidth=3, color='orange')\n",
    "    plt.plot(series.values, label = \"Data\",color='green')\n",
    "    error = mean_absolute_percentage_error(series.values, model.result[:len(series)])\n",
    "    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n",
    "    \n",
    "    if plot_anomalies:\n",
    "        anomalies = np.array([np.NaN]*len(series))\n",
    "        anomalies[series.values<model.LowerBond[:len(series)]] = \\\n",
    "            series.values[series.values<model.LowerBond[:len(series)]]\n",
    "        anomalies[series.values>model.UpperBond[:len(series)]] = \\\n",
    "            series.values[series.values>model.UpperBond[:len(series)]]\n",
    "        plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
    "    \n",
    "    if plot_intervals:\n",
    "        plt.plot(model.UpperBond, \"r--\", alpha=.8, label = \"Up/Low confidence\")\n",
    "        plt.plot(model.LowerBond, \"r--\", alpha=.8)\n",
    "        plt.fill_between(x=range(0,len(model.result)), y1=model.UpperBond, \n",
    "                         y2=model.LowerBond, alpha=1, color = \"grey\")    \n",
    "        \n",
    "    plt.vlines(len(series), ymin=min(model.LowerBond), ymax=max(model.UpperBond), linestyles='dashed')\n",
    "    plt.axvspan(len(series)-12, len(model.result), alpha=0.3, color='lightgrey')\n",
    "    plt.grid(True)\n",
    "    plt.axis('tight')\n",
    "    plt.legend(loc=\"best\", fontsize=13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotHoltWinters(series=model.series, plot_intervals=False, plot_anomalies=False, slen=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHoltWinters_zoom(series, zoom):\n",
    "    \"\"\"\n",
    "        series - dataset with timeseries\n",
    "        plot_intervals - show confidence intervals\n",
    "        plot_anomalies - show anomalies \n",
    "    \"\"\"\n",
    "    r = randint(a=zoom/2,b=len(series)-zoom/2)\n",
    "    r1 =int(r-zoom/2)\n",
    "    r2=int(r+zoom/2)\n",
    "    x=list(np.arange(0,zoom))\n",
    "    model.series.index[r1:r2]\n",
    "    res=model.result[r1:r2]\n",
    "    val=list(model.series.values[r1:r2])\n",
    "    start=model.series.index[r1]\n",
    "    end=model.series.index[2]\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(res, label = \"Model\")\n",
    "    plt.plot(val, label = \"Data\")\n",
    "    error = mean_absolute_percentage_error(np.array(val), res)\n",
    "    plt.title(\"Mean Absolute Percentage Error: {:.2f}%\\nRandom periode : {} to {}\".format(error,start,end))\n",
    "    plt.legend(loc=\"best\", fontsize=13);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 5))\n",
    "plt.plot(model.PredictedDeviation)\n",
    "plt.grid(True)\n",
    "plt.axis('tight')\n",
    "plt.title(\"Brutlag's predicted deviation\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_weight_moving_average(signal, period):\n",
    "    buffer = [np.nan] * period\n",
    "    for i in range(period, len(signal)):\n",
    "        buffer.append(\n",
    "            (signal[i - period : i] * (np.arange(period) + 1)).sum()\n",
    "            / (np.arange(period) + 1).sum()\n",
    "        )\n",
    "    return buffer\n",
    "\n",
    "data=df_p9_first_correction.copy()\n",
    "\n",
    "print(len(data))\n",
    "\n",
    "x=data['Datetime']\n",
    "y1=data['Correction_consommation']\n",
    "y2=linear_weight_moving_average(y1,7)\n",
    "y4=linear_weight_moving_average(y1,365)\n",
    "\n",
    "trace1 = go.Line(x=x,\n",
    "                  y=y1,\n",
    "                  opacity = 0.8,\n",
    "                  name='Consommation corrigée',\n",
    "                  marker=dict(color='red'))\n",
    "\n",
    "trace2 = go.Line(x=x,\n",
    "                  y=y2,\n",
    "                  opacity = 0.8,\n",
    "                  name='LMA(7)',\n",
    "                  marker=dict(color='green'))\n",
    "\n",
    "trace3 = go.Line(x=x,\n",
    "                  y=y4,\n",
    "                  opacity = 0.8,\n",
    "                  name='LMA(365)',\n",
    "                  marker=dict(color='blue'))\n",
    "\n",
    "\n",
    "data = [trace1,trace2,trace3]\n",
    "\n",
    "layout = go.Layout(title=dict(text=\"<b><b>\",\n",
    "                              font=dict(family='Linux Biolinum O',size=18,color='black')),\n",
    "                   yaxis=dict(title='KW/h'),\n",
    "                   xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=7,\n",
    "                     label='7d',\n",
    "                     step='day',\n",
    "                     stepmode='backward'),\n",
    "                dict(count=1,\n",
    "                     label='1y',\n",
    "                     step='year',\n",
    "                     stepmode='backward'),\n",
    "                dict(step='all')\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(\n",
    "            visible = True\n",
    "        ),\n",
    "        type='date'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "pyo.iplot(fig)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
